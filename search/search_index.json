{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"EU ETS Data Client \u00b6 etswatch provides a Python client for retrieving the latest data on the EU ETS market and its participants To install the library you can run: pip install etswatch To then download all of the EUTL accounts data you can run: python -m etswatch.deployment download-all-accounts-data N.b. this will create a sub-directory called data in the directory that you run the command Long-Term Average Price \u00b6 Candle-Stick Chart for Last 8-Weeks \u00b6","title":"Home"},{"location":"#eu-ets-data-client","text":"etswatch provides a Python client for retrieving the latest data on the EU ETS market and its participants To install the library you can run: pip install etswatch To then download all of the EUTL accounts data you can run: python -m etswatch.deployment download-all-accounts-data N.b. this will create a sub-directory called data in the directory that you run the command","title":"EU ETS Data Client"},{"location":"#long-term-average-price","text":"","title":"Long-Term Average Price"},{"location":"#candle-stick-chart-for-last-8-weeks","text":"","title":"Candle-Stick Chart for Last 8-Weeks"},{"location":"00-documentation/","text":"Documentation Generation \u00b6 #exports import json import junix import pandas as pd from html.parser import HTMLParser from nbdev.export2html import convert_md import os import codecs from ipypb import track from warnings import warn from distutils.dir_util import copy_tree User Inputs \u00b6 dev_nbs_dir = '.' docs_dir = '../docs' docs_nb_img_dir = f ' { docs_dir } /img/nbs' nb_img_dir = '../img/nbs' Converting Notebooks to Markdown \u00b6 #exports def encode_file_as_utf8 ( fp ): with codecs . open ( fp , 'r' ) as file : contents = file . read ( 1048576 ) file . close () if not contents : pass else : with codecs . open ( fp , 'w' , 'utf-8' ) as file : file . write ( contents ) def convert_nbs_to_md ( nbs_dir , docs_nb_img_dir , docs_dir ): nb_files = [ f for f in os . listdir ( nbs_dir ) if f [ - 6 :] == '.ipynb' ] for nb_file in track ( nb_files ): nb_fp = f ' { nbs_dir } / { nb_file } ' try : junix . export_images ( nb_fp , docs_nb_img_dir ) except : warn ( f 'images were failed to be exported for { nb_fp } ' ) convert_md ( nb_fp , docs_dir , img_path = f ' { docs_nb_img_dir } /' , jekyll = False ) md_fp = docs_dir + '/' + nb_file . replace ( '.ipynb' , '' ) + '.md' encode_file_as_utf8 ( md_fp ) for nbs_dir in [ dev_nbs_dir ]: convert_nbs_to_md ( nbs_dir , docs_nb_img_dir , docs_dir ) 100% 4/4 [00:01 < 00:01, 0.32s/it] Cleaning Markdown Tables \u00b6 #exports class MyHTMLParser ( HTMLParser ): def __init__ ( self ): super () . __init__ () self . tags = [] def handle_starttag ( self , tag , attrs ): self . tags . append ( self . get_starttag_text ()) def handle_endtag ( self , tag ): self . tags . append ( f \"</ { tag } >\" ) get_substring_idxs = lambda string , substring : [ num for num in range ( len ( string ) - len ( substring ) + 1 ) if string [ num : num + len ( substring )] == substring ] def convert_df_to_md ( df ): idx_col = df . columns [ 0 ] df = df . set_index ( idx_col ) if idx_col == 'Unnamed: 0' : df . index . name = '' table_md = df . to_markdown () return table_md def extract_div_to_md_table ( start_idx , end_idx , table_and_div_tags , file_txt ): n_start_divs_before = table_and_div_tags [: start_idx ] . count ( '<div>' ) n_end_divs_before = table_and_div_tags [: end_idx ] . count ( '</div>' ) div_start_idx = get_substring_idxs ( file_txt , '<div>' )[ n_start_divs_before - 1 ] div_end_idx = get_substring_idxs ( file_txt , '</div>' )[ n_end_divs_before ] div_txt = file_txt [ div_start_idx : div_end_idx ] potential_dfs = pd . read_html ( div_txt ) assert len ( potential_dfs ) == 1 , 'Multiple tables were found when there can be only one' df = potential_dfs [ 0 ] md_table = convert_df_to_md ( df ) return div_txt , md_table def extract_div_to_md_tables ( md_fp ): with open ( md_fp , 'r' ) as f : file_txt = f . read () parser = MyHTMLParser () parser . feed ( file_txt ) table_and_div_tags = [ tag for tag in parser . tags if tag in [ '<div>' , '</div>' , '<table border=\"1\" class=\"dataframe\">' , '</table>' ]] table_start_tag_idxs = [ i for i , tag in enumerate ( table_and_div_tags ) if tag == '<table border=\"1\" class=\"dataframe\">' ] table_end_tag_idxs = [ table_start_tag_idx + table_and_div_tags [ table_start_tag_idx :] . index ( '</table>' ) for table_start_tag_idx in table_start_tag_idxs ] div_to_md_tables = [] for start_idx , end_idx in zip ( table_start_tag_idxs , table_end_tag_idxs ): div_txt , md_table = extract_div_to_md_table ( start_idx , end_idx , table_and_div_tags , file_txt ) div_to_md_tables += [( div_txt , md_table )] return div_to_md_tables def clean_md_file_tables ( md_fp ): div_to_md_tables = extract_div_to_md_tables ( md_fp ) with open ( md_fp , 'r' ) as f : md_file_text = f . read () for div_txt , md_txt in div_to_md_tables : md_file_text = md_file_text . replace ( div_txt , md_txt ) with open ( md_fp , 'w' ) as f : f . write ( md_file_text ) return md_fps = [ f ' { docs_dir } / { f } ' for f in os . listdir ( docs_dir ) if f [ - 3 :] == '.md' if f != '00-documentation.md' ] for md_fp in md_fps : div_to_md_tables = clean_md_file_tables ( md_fp ) Cleaning Image Paths \u00b6 #exports def clean_md_file_img_fps ( md_fp ): with open ( md_fp , 'r' ) as f : md_file_text = f . read () md_file_text = md_file_text . replace ( '../docs/img/nbs' , 'img/nbs' ) with open ( md_fp , 'w' ) as f : f . write ( md_file_text ) return for md_fp in md_fps : clean_md_file_img_fps ( md_fp )","title":"Documentation Generation"},{"location":"00-documentation/#documentation-generation","text":"#exports import json import junix import pandas as pd from html.parser import HTMLParser from nbdev.export2html import convert_md import os import codecs from ipypb import track from warnings import warn from distutils.dir_util import copy_tree","title":"Documentation Generation"},{"location":"00-documentation/#user-inputs","text":"dev_nbs_dir = '.' docs_dir = '../docs' docs_nb_img_dir = f ' { docs_dir } /img/nbs' nb_img_dir = '../img/nbs'","title":"User Inputs"},{"location":"00-documentation/#converting-notebooks-to-markdown","text":"#exports def encode_file_as_utf8 ( fp ): with codecs . open ( fp , 'r' ) as file : contents = file . read ( 1048576 ) file . close () if not contents : pass else : with codecs . open ( fp , 'w' , 'utf-8' ) as file : file . write ( contents ) def convert_nbs_to_md ( nbs_dir , docs_nb_img_dir , docs_dir ): nb_files = [ f for f in os . listdir ( nbs_dir ) if f [ - 6 :] == '.ipynb' ] for nb_file in track ( nb_files ): nb_fp = f ' { nbs_dir } / { nb_file } ' try : junix . export_images ( nb_fp , docs_nb_img_dir ) except : warn ( f 'images were failed to be exported for { nb_fp } ' ) convert_md ( nb_fp , docs_dir , img_path = f ' { docs_nb_img_dir } /' , jekyll = False ) md_fp = docs_dir + '/' + nb_file . replace ( '.ipynb' , '' ) + '.md' encode_file_as_utf8 ( md_fp ) for nbs_dir in [ dev_nbs_dir ]: convert_nbs_to_md ( nbs_dir , docs_nb_img_dir , docs_dir ) 100% 4/4 [00:01 < 00:01, 0.32s/it]","title":"Converting Notebooks to Markdown"},{"location":"00-documentation/#cleaning-markdown-tables","text":"#exports class MyHTMLParser ( HTMLParser ): def __init__ ( self ): super () . __init__ () self . tags = [] def handle_starttag ( self , tag , attrs ): self . tags . append ( self . get_starttag_text ()) def handle_endtag ( self , tag ): self . tags . append ( f \"</ { tag } >\" ) get_substring_idxs = lambda string , substring : [ num for num in range ( len ( string ) - len ( substring ) + 1 ) if string [ num : num + len ( substring )] == substring ] def convert_df_to_md ( df ): idx_col = df . columns [ 0 ] df = df . set_index ( idx_col ) if idx_col == 'Unnamed: 0' : df . index . name = '' table_md = df . to_markdown () return table_md def extract_div_to_md_table ( start_idx , end_idx , table_and_div_tags , file_txt ): n_start_divs_before = table_and_div_tags [: start_idx ] . count ( '<div>' ) n_end_divs_before = table_and_div_tags [: end_idx ] . count ( '</div>' ) div_start_idx = get_substring_idxs ( file_txt , '<div>' )[ n_start_divs_before - 1 ] div_end_idx = get_substring_idxs ( file_txt , '</div>' )[ n_end_divs_before ] div_txt = file_txt [ div_start_idx : div_end_idx ] potential_dfs = pd . read_html ( div_txt ) assert len ( potential_dfs ) == 1 , 'Multiple tables were found when there can be only one' df = potential_dfs [ 0 ] md_table = convert_df_to_md ( df ) return div_txt , md_table def extract_div_to_md_tables ( md_fp ): with open ( md_fp , 'r' ) as f : file_txt = f . read () parser = MyHTMLParser () parser . feed ( file_txt ) table_and_div_tags = [ tag for tag in parser . tags if tag in [ '<div>' , '</div>' , '<table border=\"1\" class=\"dataframe\">' , '</table>' ]] table_start_tag_idxs = [ i for i , tag in enumerate ( table_and_div_tags ) if tag == '<table border=\"1\" class=\"dataframe\">' ] table_end_tag_idxs = [ table_start_tag_idx + table_and_div_tags [ table_start_tag_idx :] . index ( '</table>' ) for table_start_tag_idx in table_start_tag_idxs ] div_to_md_tables = [] for start_idx , end_idx in zip ( table_start_tag_idxs , table_end_tag_idxs ): div_txt , md_table = extract_div_to_md_table ( start_idx , end_idx , table_and_div_tags , file_txt ) div_to_md_tables += [( div_txt , md_table )] return div_to_md_tables def clean_md_file_tables ( md_fp ): div_to_md_tables = extract_div_to_md_tables ( md_fp ) with open ( md_fp , 'r' ) as f : md_file_text = f . read () for div_txt , md_txt in div_to_md_tables : md_file_text = md_file_text . replace ( div_txt , md_txt ) with open ( md_fp , 'w' ) as f : f . write ( md_file_text ) return md_fps = [ f ' { docs_dir } / { f } ' for f in os . listdir ( docs_dir ) if f [ - 3 :] == '.md' if f != '00-documentation.md' ] for md_fp in md_fps : div_to_md_tables = clean_md_file_tables ( md_fp )","title":"Cleaning Markdown Tables"},{"location":"00-documentation/#cleaning-image-paths","text":"#exports def clean_md_file_img_fps ( md_fp ): with open ( md_fp , 'r' ) as f : md_file_text = f . read () md_file_text = md_file_text . replace ( '../docs/img/nbs' , 'img/nbs' ) with open ( md_fp , 'w' ) as f : f . write ( md_file_text ) return for md_fp in md_fps : clean_md_file_img_fps ( md_fp )","title":"Cleaning Image Paths"},{"location":"01-prices/","text":"Title \u00b6 #exports import json import pandas as pd import matplotlib.pyplot as plt import mplfinance as mpf import FEAutils as hlp import os import dotenv import requests from IPython.display import JSON dotenv . load_dotenv () api_key = os . getenv ( 'QUANDL_API_KEY' ) #exports def get_ets_mkt_data (): # Constructing endpoint api_root = 'https://www.quandl.com/api/v3/datasets' quandl_code = 'CHRIS/ICE_C1' endpoint_url = f ' { api_root } / { quandl_code } .json' # Making the data request params = { 'api_key' : api_key } r = requests . get ( endpoint_url , params = params ) # Converting to a dataframe dataset = r . json ()[ 'dataset' ] df = pd . DataFrame ( dataset [ 'data' ], columns = dataset [ 'column_names' ]) # Cleaning the dataframe df . columns = df . columns . str . lower () . str . replace ( '.' , '' ) . str . replace ( ' ' , '_' ) df = df . rename ( columns = { 'date' : 'datetime' }) . set_index ( 'datetime' ) df [ 'close' ] = df [ 'open' ] + df [ 'change' ] df . index = pd . to_datetime ( df . index ) df = df . sort_index () return df %% time df_ets = get_ets_mkt_data () df_ets . head () Wall time: 1.4 s C:\\Users\\Ayrto\\anaconda3\\envs\\ETSWatch\\lib\\site-packages\\ipykernel_launcher.py:17: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True. ('Unnamed: 0_level_0', 'datetime') ('open', 'Unnamed: 1_level_1') ('high', 'Unnamed: 2_level_1') ('low', 'Unnamed: 3_level_1') ('settle', 'Unnamed: 4_level_1') ('change', 'Unnamed: 5_level_1') ('wave', 'Unnamed: 6_level_1') ('volume', 'Unnamed: 7_level_1') ('prev_day_open_interest', 'Unnamed: 8_level_1') ('efp_volume', 'Unnamed: 9_level_1') ('efs_volume', 'Unnamed: 10_level_1') ('block_volume', 'Unnamed: 11_level_1') ('close', 'Unnamed: 12_level_1') 2008-04-08 24.87 24.87 24.87 24.87 24.87 nan nan 0 nan nan nan 49.74 2008-04-09 25.05 25.05 25.05 25.05 0.18 nan nan 0 nan nan nan 25.23 2008-04-10 25.79 25.79 25.79 25.79 0.74 nan nan 0 nan nan nan 26.53 2008-04-11 25.7 25.7 25.7 25.7 -0.09 nan nan 0 nan nan nan 25.61 2008-04-14 26.34 26.34 26.34 26.34 0.64 nan nan 0 nan nan nan 26.98 #exports def plot_long_term_avg ( df ): fig , ax = plt . subplots ( dpi = 250 , figsize = ( 10 , 4 )) df [ 'settle' ] . plot ( ax = ax , color = '#AE0019' , linewidth = 1 ) ax . set_ylim ( 0 ) ax . set_xlim ( df . index . min (), df . index . max ()) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Price (EUR/tonne CO2)' ) hlp . hide_spines ( ax ) return fig , ax fig , ax = plot_long_term_avg ( df_ets ) #exports def plot_ohlc_vol ( df ): fig , axs = plt . subplots ( dpi = 250 , nrows = 2 , figsize = ( 8 , 8 )) mpf . plot ( df , type = 'candle' , ax = axs [ 0 ], volume = axs [ 1 ], show_nontrading = True , style = 'sas' ) ax = axs [ 0 ] ax . set_xticks ([]) ax . set_xticklabels ([]) ax . set_ylabel ( 'Price (EUR/tonne CO2)' ) ax . yaxis . set_label_position ( 'left' ) ax . yaxis . tick_left () hlp . hide_spines ( ax , positions = [ 'top' , 'bottom' , 'right' ]) ax = axs [ 1 ] ax . set_ylabel ( 'Volume (tonne CO2)' ) hlp . hide_spines ( ax , positions = [ 'top' , 'right' ]) return fig , axs def plot_recent_ohlc_vol ( df , weeks = 8 , latest_date = None ): if latest_date is None : latest_date = df . index . max () earliest_date = latest_date - pd . Timedelta ( weeks = weeks ) cols = [ 'open' , 'high' , 'low' , 'close' , 'volume' ] df_recent = df [ cols ] . sort_index () . dropna () . loc [ earliest_date : latest_date ] fig , axs = plot_ohlc_vol ( df_recent ) return fig , axs fig , axs = plot_recent_ohlc_vol ( df_ets ) fig , ax = plt . subplots ( dpi = 150 ) df_ets [ 'volume' ] . resample ( '14D' ) . mean () . dropna ()[ '2012' :] . plot ( ax = ax ) hlp . hide_spines ( ax ) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Average Daily Volume (tonne CO2)' ) Text(0, 0.5, 'Average Daily Volume (tonne CO2)') fig , ax = plt . subplots ( dpi = 150 ) df_ets [ 'volume' ] . groupby ( df_ets . index . dayofyear ) . mean () . plot () hlp . hide_spines ( ax ) ax . set_xlim ( 0 , 365 ) ax . set_ylim ( 0 ) ax . set_xlabel ( 'Day of the Year' ) ax . set_ylabel ( 'Average Daily Volume (tonne CO2)' ) Text(0, 0.5, 'Average Daily Volume (tonne CO2)')","title":"Market Prices"},{"location":"01-prices/#title","text":"#exports import json import pandas as pd import matplotlib.pyplot as plt import mplfinance as mpf import FEAutils as hlp import os import dotenv import requests from IPython.display import JSON dotenv . load_dotenv () api_key = os . getenv ( 'QUANDL_API_KEY' ) #exports def get_ets_mkt_data (): # Constructing endpoint api_root = 'https://www.quandl.com/api/v3/datasets' quandl_code = 'CHRIS/ICE_C1' endpoint_url = f ' { api_root } / { quandl_code } .json' # Making the data request params = { 'api_key' : api_key } r = requests . get ( endpoint_url , params = params ) # Converting to a dataframe dataset = r . json ()[ 'dataset' ] df = pd . DataFrame ( dataset [ 'data' ], columns = dataset [ 'column_names' ]) # Cleaning the dataframe df . columns = df . columns . str . lower () . str . replace ( '.' , '' ) . str . replace ( ' ' , '_' ) df = df . rename ( columns = { 'date' : 'datetime' }) . set_index ( 'datetime' ) df [ 'close' ] = df [ 'open' ] + df [ 'change' ] df . index = pd . to_datetime ( df . index ) df = df . sort_index () return df %% time df_ets = get_ets_mkt_data () df_ets . head () Wall time: 1.4 s C:\\Users\\Ayrto\\anaconda3\\envs\\ETSWatch\\lib\\site-packages\\ipykernel_launcher.py:17: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True. ('Unnamed: 0_level_0', 'datetime') ('open', 'Unnamed: 1_level_1') ('high', 'Unnamed: 2_level_1') ('low', 'Unnamed: 3_level_1') ('settle', 'Unnamed: 4_level_1') ('change', 'Unnamed: 5_level_1') ('wave', 'Unnamed: 6_level_1') ('volume', 'Unnamed: 7_level_1') ('prev_day_open_interest', 'Unnamed: 8_level_1') ('efp_volume', 'Unnamed: 9_level_1') ('efs_volume', 'Unnamed: 10_level_1') ('block_volume', 'Unnamed: 11_level_1') ('close', 'Unnamed: 12_level_1') 2008-04-08 24.87 24.87 24.87 24.87 24.87 nan nan 0 nan nan nan 49.74 2008-04-09 25.05 25.05 25.05 25.05 0.18 nan nan 0 nan nan nan 25.23 2008-04-10 25.79 25.79 25.79 25.79 0.74 nan nan 0 nan nan nan 26.53 2008-04-11 25.7 25.7 25.7 25.7 -0.09 nan nan 0 nan nan nan 25.61 2008-04-14 26.34 26.34 26.34 26.34 0.64 nan nan 0 nan nan nan 26.98 #exports def plot_long_term_avg ( df ): fig , ax = plt . subplots ( dpi = 250 , figsize = ( 10 , 4 )) df [ 'settle' ] . plot ( ax = ax , color = '#AE0019' , linewidth = 1 ) ax . set_ylim ( 0 ) ax . set_xlim ( df . index . min (), df . index . max ()) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Price (EUR/tonne CO2)' ) hlp . hide_spines ( ax ) return fig , ax fig , ax = plot_long_term_avg ( df_ets ) #exports def plot_ohlc_vol ( df ): fig , axs = plt . subplots ( dpi = 250 , nrows = 2 , figsize = ( 8 , 8 )) mpf . plot ( df , type = 'candle' , ax = axs [ 0 ], volume = axs [ 1 ], show_nontrading = True , style = 'sas' ) ax = axs [ 0 ] ax . set_xticks ([]) ax . set_xticklabels ([]) ax . set_ylabel ( 'Price (EUR/tonne CO2)' ) ax . yaxis . set_label_position ( 'left' ) ax . yaxis . tick_left () hlp . hide_spines ( ax , positions = [ 'top' , 'bottom' , 'right' ]) ax = axs [ 1 ] ax . set_ylabel ( 'Volume (tonne CO2)' ) hlp . hide_spines ( ax , positions = [ 'top' , 'right' ]) return fig , axs def plot_recent_ohlc_vol ( df , weeks = 8 , latest_date = None ): if latest_date is None : latest_date = df . index . max () earliest_date = latest_date - pd . Timedelta ( weeks = weeks ) cols = [ 'open' , 'high' , 'low' , 'close' , 'volume' ] df_recent = df [ cols ] . sort_index () . dropna () . loc [ earliest_date : latest_date ] fig , axs = plot_ohlc_vol ( df_recent ) return fig , axs fig , axs = plot_recent_ohlc_vol ( df_ets ) fig , ax = plt . subplots ( dpi = 150 ) df_ets [ 'volume' ] . resample ( '14D' ) . mean () . dropna ()[ '2012' :] . plot ( ax = ax ) hlp . hide_spines ( ax ) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Average Daily Volume (tonne CO2)' ) Text(0, 0.5, 'Average Daily Volume (tonne CO2)') fig , ax = plt . subplots ( dpi = 150 ) df_ets [ 'volume' ] . groupby ( df_ets . index . dayofyear ) . mean () . plot () hlp . hide_spines ( ax ) ax . set_xlim ( 0 , 365 ) ax . set_ylim ( 0 ) ax . set_xlabel ( 'Day of the Year' ) ax . set_ylabel ( 'Average Daily Volume (tonne CO2)' ) Text(0, 0.5, 'Average Daily Volume (tonne CO2)')","title":"Title"},{"location":"02-installation-allocations/","text":"Installation Allocations \u00b6 #exports import pandas as pd import numpy as np import requests from bs4 import BeautifulSoup as bs import urllib.parse as urlparse from urllib.parse import parse_qs import re from warnings import warn from ipypb import track from IPython.display import JSON Retrieving Installation Table Links \u00b6 In this first section we'll set out to retrieve the urls for the installations databases across different ETS phases and each country. We'll start by retrieving the raw results for a single country #exports def get_country_raw_search ( country_code = 'AT' ): url = 'https://ec.europa.eu/clima/ets/nap.do' params = { 'languageCode' : 'en' , 'nap.registryCodeArray' : country_code , 'periodCode' : '-1' , 'search' : 'Search' , 'currentSortSettings' : '' } r = requests . get ( url , params = params ) return r r = get_country_raw_search ( 'IS' ) r <Response [200]> From the response we can extract a table containing the relevant information on the installation databases #exports def extract_search_df ( r ): soup = bs ( r . text ) results_table = soup . find ( 'table' , attrs = { 'id' : 'tblNapSearchResult' }) df_search = ( pd . read_html ( str ( results_table )) [ 0 ] . iloc [ 2 :, : - 3 ] . reset_index ( drop = True ) . T . set_index ( 0 ) . T . reset_index ( drop = True ) . rename ( columns = { 'National Administrator' : 'country' , 'EU ETS Phase' : 'phase' , 'For issuance to not new entrants' : 'non_new_entrants' , 'From NER' : 'new_entrants_reserve' }) ) df_search [ 'installations_link' ] = [ 'https://ec.europa.eu/' + a [ 'href' ] for a in soup . findAll ( 'a' , text = re . compile ( 'Installations linked to this Allocation Table' ))] return df_search df_search_country = extract_search_df ( r ) df_search_country country phase non_new_entrants new_entrants_reserve installations_link 0 Iceland Phase 3 (2013-2020) 11527090 172828 https://ec.europa.eu//clima/ets/napInstallatio... It's all good doing this for Austria but we want European-wide coverage. We can identify the countries we can query from the option box on the main search page. #exports def get_country_codes (): r = get_country_raw_search () soup = bs ( r . text ) registry_code_to_country = { option [ 'value' ]: option . text for option in soup . find ( 'select' , attrs = { 'name' : 'nap.registryCodeArray' }) . findAll ( 'option' ) } return registry_code_to_country registry_code_to_country = get_country_codes () JSON ([ registry_code_to_country ]) <IPython.core.display.JSON object> We can now use these to repeat our search for each country and concatenate the results #exports def get_installation_links_dataframe (): df_search = pd . DataFrame () for registry_code in registry_code_to_country . keys (): r = get_country_raw_search ( registry_code ) df_search_country = extract_search_df ( r ) df_search = df_search . append ( df_search_country ) df_search = df_search . reset_index ( drop = True ) null_values_present = df_search . isnull () . sum () . sum () > 0 if null_values_present == True : warn ( 'There are null values present in the dataframe' ) return df_search df_search = get_installation_links_dataframe () df_search . head () country phase non_new_entrants new_entrants_reserve installations_link 0 Austria Phase 1 (2005-2007) 97791309 990150 https://ec.europa.eu//clima/ets/napInstallatio... 1 Austria Phase 2 (2008-2012) 160218569 2000000 https://ec.europa.eu//clima/ets/napInstallatio... 2 Austria Phase 3 (2013-2020) 160295499 1893669 https://ec.europa.eu//clima/ets/napInstallatio... 3 Belgium Phase 1 (2005-2007) 178690906 7653297 https://ec.europa.eu//clima/ets/napInstallatio... 4 Belgium Phase 2 (2008-2012) 283317829 9153852 https://ec.europa.eu//clima/ets/napInstallatio... Retrieving Installation Allocation Data \u00b6 In this section we'll start by separating the root url and the parameters from each of the installation links #exports def get_url_root_and_params ( installations_link ): url_root = installations_link . split ( '?' )[ 0 ] parsed = urlparse . urlparse ( installations_link ) params = { k : v [ 0 ] for k , v in parse_qs ( parsed . query ) . items ()} return url_root , params installations_link = df_search . loc [ 0 , 'installations_link' ] url_root , params = get_url_root_and_params ( installations_link ) JSON ( params ) <IPython.core.display.JSON object> We also need to pass the page number to the parameters, to do this we need to first know how many pages there are #exports def get_num_pages ( root_url , params ): soup = bs ( requests . get ( root_url , params = params ) . text ) soup_pn = soup . find ( 'input' , attrs = { 'name' : 'resultList.lastPageNumber' }) if soup_pn is not None : num_pages = int ( soup_pn [ 'value' ]) else : num_pages = 1 return num_pages num_pages = get_num_pages ( root_url , params ) num_pages 11 We're now ready to iterate over multiple pages and combine the results for a single ETS phase in a single country #exports def extract_installation_allocations_df ( r ): soup = bs ( r . text ) table = soup . find ( 'table' , attrs = { 'id' : 'tblNapList' }) df_installation_allocations = ( pd . read_html ( str ( table )) [ 0 ] . drop ([ 0 , 1 ]) . reset_index ( drop = True ) . T . set_index ( 0 ) . T . drop ( columns = [ 'Options' ]) ) return df_installation_allocations def retry_request ( root_url , params , n_retries = 5 , ** kwargs ): for i in range ( n_retries ): try : r = requests . get ( root_url , params = params , ** kwargs ) return r except Exception as e : continue raise e def get_installation_allocations_df ( root_url , params , n_retries = 5 ): df_installation_allocations = pd . DataFrame () num_pages = get_num_pages ( root_url , params ) params [ 'nextList' ] = 'Next' for page_num in range ( num_pages ): params [ 'resultList.currentPageNumber' ] = page_num r = retry_request ( root_url , params , n_retries = n_retries ) df_installation_allocations_page = extract_installation_allocations_df ( r ) df_installation_allocations = df_installation_allocations . append ( df_installation_allocations_page ) df_installation_allocations = df_installation_allocations . reset_index ( drop = True ) return df_installation_allocations df_installation_allocations = get_installation_allocations_df ( root_url , params ) print ( f 'DataFrame shape: { df_installation_allocations . shape } ' ) df_installation_allocations . head ( 3 ) DataFrame shape: (201, 11) Installation ID Installation Name Address City Account Holder Name Account Status Permit ID Latest Update 2005 2006 2007 Status 0 1 Baumit Baustoffe Bad Ischl Bad Ischl Calmit GmbH open IKA119 2009-05-08 09:13:58 44894 44894 44894 Active 1 2 Breitenfelder Edelstahl Mitterdorf Mitterdorf Breitenfeld Edelstahl AG open IES069 2009-05-08 09:13:58 8492 8492 8492 Active 2 3 Ziegelwerk Danreiter Ried im Innkreis Ried Ziegelwerk Danreiter GmbH & Co KG open IZI155 2009-05-08 09:13:58 7397 7397 7397 Active The next step is to repeat this for all countries and ETS phases, then combine the resulting dataframes #exports def get_all_installation_allocations_df ( df_search ): col_renaming_map = { 'Installation ID' : 'installation_id' , 'Installation Name' : 'installation_name' , 'Address City' : 'installation_city' , 'Account Holder Name' : 'account_holder' , 'Account Status' : 'account_status' , 'Permit ID' : 'permit_id' , 'Status' : 'status' } df_installation_allocations = pd . DataFrame () # Retrieving raw data for country in track ( df_search [ 'country' ] . unique ()): df_installation_allocations_country = pd . DataFrame () country_installations_links = df_search . loc [ df_search [ 'country' ] == country , 'installations_link' ] for installations_link in track ( country_installations_links , label = country ): url_root , params = get_url_root_and_params ( installations_link ) df_installation_allocations_country_phase = get_installation_allocations_df ( root_url , params ) if df_installation_allocations_country . size > 0 : df_installation_allocations_country = pd . merge ( df_installation_allocations_country , df_installation_allocations_country_phase , how = 'outer' , on = list ( col_renaming_map . keys ())) else : df_installation_allocations_country = df_installation_allocations_country_phase df_installation_allocations_country [ 'country' ] = country df_installation_allocations = df_installation_allocations . append ( df_installation_allocations_country ) # Collating update datetimes update_cols = df_installation_allocations . columns [ df_installation_allocations . columns . str . contains ( 'Latest Update' )] df_installation_allocations [ 'latest_update' ] = df_installation_allocations [ update_cols ] . fillna ( '' ) . max ( axis = 1 ) df_installation_allocations = df_installation_allocations . drop ( columns = update_cols ) # Renaming columns df_installation_allocations = ( df_installation_allocations . reset_index ( drop = True ) . rename ( columns = col_renaming_map ) ) # Sorting column order non_year_cols = [ 'country' ] + list ( col_renaming_map . values ()) + [ 'latest_update' ] year_cols = sorted ( list ( set ( df_installation_allocations . columns ) - set ( non_year_cols ))) df_installation_allocations = df_installation_allocations [ non_year_cols + year_cols ] # Dropping header rows idxs_to_drop = df_installation_allocations [ 'permit_id' ] . str . contains ( '\\*' ) . replace ( False , np . nan ) . dropna () . index df_installation_allocations = df_installation_allocations . drop ( idxs_to_drop ) return df_installation_allocations df_installation_allocations = get_all_installation_allocations_df ( df_search ) df_installation_allocations . head () 100% 32/32 [24:25 < 01:34, 45.78s/it] Austria 100% 3/3 [00:16 < 00:05, 5.35s/it] Belgium 100% 3/3 [00:24 < 00:10, 7.95s/it] Bulgaria 100% 2/2 [00:07 < 00:03, 3.37s/it] Croatia 100% 1/1 [00:01 < 00:01, 1.26s/it] Cyprus 100% 2/2 [00:01 < 00:01, 0.64s/it] Czech Republic 100% 3/3 [00:33 < 00:12, 11.09s/it] Denmark 100% 3/3 [00:30 < 00:11, 10.07s/it] Estonia 100% 3/3 [00:04 < 00:01, 1.21s/it] Finland 100% 3/3 [01:04 < 00:19, 21.32s/it] France 100% 3/3 [02:50 < 01:18, 56.52s/it] Germany 100% 3/3 [06:22 < 03:02, 127.24s/it] Greece 100% 3/3 [00:09 < 00:03, 3.03s/it] Hungary 100% 3/3 [00:18 < 00:07, 5.88s/it] Iceland 100% 1/1 [00:01 < 00:01, 0.63s/it] Ireland 100% 3/3 [00:08 < 00:02, 2.58s/it] Italy 100% 3/3 [02:45 < 01:22, 54.94s/it] Latvia 100% 3/3 [00:06 < 00:02, 1.88s/it] Liechtenstein 100% 2/2 [00:01 < 00:00, 0.49s/it] Lithuania 100% 3/3 [00:07 < 00:03, 2.46s/it] Luxembourg 100% 3/3 [00:02 < 00:01, 0.54s/it] Malta 100% 1/1 [00:00 < 00:00, 0.47s/it] Netherlands 100% 3/3 [00:33 < 00:15, 10.85s/it] Northern Ireland 100% 3/3 [00:01 < 00:00, 0.46s/it] Norway 100% 2/2 [00:06 < 00:03, 3.00s/it] Poland 100% 3/3 [01:48 < 00:41, 36.07s/it] Portugal 100% 3/3 [00:16 < 00:05, 5.37s/it] Romania 100% 3/3 [00:16 < 00:05, 5.45s/it] Slovakia 100% 3/3 [00:11 < 00:04, 3.60s/it] Slovenia 100% 3/3 [00:05 < 00:01, 1.74s/it] Spain 100% 3/3 [02:11 < 00:48, 43.72s/it] Sweden 100% 3/3 [01:35 < 00:30, 31.67s/it] United Kingdom 100% 3/3 [01:34 < 00:31, 31.37s/it] country installation_id installation_name installation_city account_holder account_status permit_id status latest_update 2005 ... 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 0 Austria 1 Baumit Baustoffe Bad Ischl Bad Ischl Calmit GmbH open IKA119 Active 2013-12-19 15:47:52 44894 ... 43171 43171 42159 41426 40685 39937 39180 38416 37643 36866 1 Austria 2 Breitenfelder Edelstahl Mitterdorf Mitterdorf Breitenfeld Edelstahl AG open IES069 Active 2013-12-19 15:48:16 8492 ... 26429 26429 15118 14856 14590 14322 14051 13776 13499 13221 2 Austria 3 Ziegelwerk Danreiter Ried im Innkreis Ried Ziegelwerk Danreiter GmbH & Co KG open IZI155 Active 2013-12-19 15:48:12 7397 ... 5927 5927 3494 3434 3373 3311 3248 3185 3120 3056 3 Austria 4 Wienerberger Blindenmarkt Blindenmarkt Wienerberger \u00d6sterreich GmbH closed IZI146-1 Account Closed 2009-05-08 09:13:58 8335 ... nan nan nan nan nan nan nan nan nan nan 4 Austria 5 Isomax Dekorative Laminate Wiener Neudorf Wiener Neudorf FunderMax GmbH open ICH113 Active 2013-12-19 15:47:46 24003 ... 27343 27343 26223 25697 25167 24636 24102 23566 23026 22488 A quick check reveals some issues with the dataset, for example there are several permit ids that have duplicate entries permit_ids_with_duplicate_idxs = ( df_installation_allocations [ 'permit_id' ] . value_counts () > 1 ) . replace ( False , np . nan ) . dropna () . index df_dupe_permit_ids = df_installation_allocations [ df_installation_allocations [ 'permit_id' ] . isin ( permit_ids_with_duplicate_idxs )] . sort_values ( 'permit_id' ) print ( f \"There are { df_dupe_permit_ids [ 'permit_id' ] . unique () . size } permit ids which have duplicate entries due to inconsistent values (e.g. for `installation_id`) \\n \" ) df_dupe_permit_ids . head ( 6 ) There are 109 permit ids which have duplicate entries due to inconsistent values (e.g. for `installation_id`) country installation_id installation_name installation_city account_holder account_status permit_id status latest_update 2005 ... 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 4184 France 204319 ARKEMA FRANCE- Usine de Mont Orthez ARKEMA FRANCE open 5202690 Active 2014-01-27 16:54:18 nan ... nan nan 26157 25703 25242 24779 24308 23835 23355 22873 2937 France 102 ARKEMA FRANCE - Usine de Mont MONT ARKEMA FRANCE closed 5202690 Account Closed 2012-11-29 14:18:52 32802 ... 0 0 nan nan nan nan nan nan nan nan 4165 France 204063 SOCIETE FROMAGERE DE SAINTE CECILE SAINTE-CECILE SOCIETE FROMAGERE DE SAINTE CECILE open 5301510 Active 2014-01-27 16:57:52 nan ... nan nan 3864 3458 3063 2680 2309 1949 1602 1267 3913 France 1121 SOCIETE FROMAGERE DE SAINTE CECILE Sainte C\u00e9cile BOUVIER closed 5301510 Account Closed 2009-05-08 09:13:58 7790 ... nan nan nan nan nan nan nan nan nan nan 4162 France 204060 SOCIETE FROMAGERE DE DOMFRONT Domfront SOCIETE FROMAGERE DE DOMFRONT open 5302209 Active 2014-01-27 16:54:24 nan ... nan nan 5780 5173 4582 4009 3454 2916 2396 1895 2961 France 127 SOCIETE FROMAGERE DE DOMFRONT Domfront SCOTTO closed 5302209 Account Closed 2009-05-08 09:13:58 11326 ... nan nan nan nan nan nan nan nan nan nan Lastly we'll wrap these steps in a function that will retrieve/download our data to/from the specified directory #exports def get_installation_allocations_df ( data_dir = 'data' , redownload = False ): if not os . path . exists ( data_dir ): os . makedirs ( data_dir ) if redownload == True : df_search = get_installation_links_dataframe () df_installation_allocations = get_all_installation_allocations_df ( df_search ) df_installation_allocations . to_csv ( f ' { data_dir } /installation_allocations.csv' , index = False ) else : df_installation_allocations = pd . read_csv ( f ' { data_dir } /installation_allocations.csv' ) return df_installation_allocations redownload_installation_allocations = False df_installation_allocations = get_installation_allocations_df ( data_dir = '../data' , redownload = redownload_installation_allocations )","title":"Installations"},{"location":"02-installation-allocations/#installation-allocations","text":"#exports import pandas as pd import numpy as np import requests from bs4 import BeautifulSoup as bs import urllib.parse as urlparse from urllib.parse import parse_qs import re from warnings import warn from ipypb import track from IPython.display import JSON","title":"Installation Allocations"},{"location":"02-installation-allocations/#retrieving-installation-table-links","text":"In this first section we'll set out to retrieve the urls for the installations databases across different ETS phases and each country. We'll start by retrieving the raw results for a single country #exports def get_country_raw_search ( country_code = 'AT' ): url = 'https://ec.europa.eu/clima/ets/nap.do' params = { 'languageCode' : 'en' , 'nap.registryCodeArray' : country_code , 'periodCode' : '-1' , 'search' : 'Search' , 'currentSortSettings' : '' } r = requests . get ( url , params = params ) return r r = get_country_raw_search ( 'IS' ) r <Response [200]> From the response we can extract a table containing the relevant information on the installation databases #exports def extract_search_df ( r ): soup = bs ( r . text ) results_table = soup . find ( 'table' , attrs = { 'id' : 'tblNapSearchResult' }) df_search = ( pd . read_html ( str ( results_table )) [ 0 ] . iloc [ 2 :, : - 3 ] . reset_index ( drop = True ) . T . set_index ( 0 ) . T . reset_index ( drop = True ) . rename ( columns = { 'National Administrator' : 'country' , 'EU ETS Phase' : 'phase' , 'For issuance to not new entrants' : 'non_new_entrants' , 'From NER' : 'new_entrants_reserve' }) ) df_search [ 'installations_link' ] = [ 'https://ec.europa.eu/' + a [ 'href' ] for a in soup . findAll ( 'a' , text = re . compile ( 'Installations linked to this Allocation Table' ))] return df_search df_search_country = extract_search_df ( r ) df_search_country country phase non_new_entrants new_entrants_reserve installations_link 0 Iceland Phase 3 (2013-2020) 11527090 172828 https://ec.europa.eu//clima/ets/napInstallatio... It's all good doing this for Austria but we want European-wide coverage. We can identify the countries we can query from the option box on the main search page. #exports def get_country_codes (): r = get_country_raw_search () soup = bs ( r . text ) registry_code_to_country = { option [ 'value' ]: option . text for option in soup . find ( 'select' , attrs = { 'name' : 'nap.registryCodeArray' }) . findAll ( 'option' ) } return registry_code_to_country registry_code_to_country = get_country_codes () JSON ([ registry_code_to_country ]) <IPython.core.display.JSON object> We can now use these to repeat our search for each country and concatenate the results #exports def get_installation_links_dataframe (): df_search = pd . DataFrame () for registry_code in registry_code_to_country . keys (): r = get_country_raw_search ( registry_code ) df_search_country = extract_search_df ( r ) df_search = df_search . append ( df_search_country ) df_search = df_search . reset_index ( drop = True ) null_values_present = df_search . isnull () . sum () . sum () > 0 if null_values_present == True : warn ( 'There are null values present in the dataframe' ) return df_search df_search = get_installation_links_dataframe () df_search . head () country phase non_new_entrants new_entrants_reserve installations_link 0 Austria Phase 1 (2005-2007) 97791309 990150 https://ec.europa.eu//clima/ets/napInstallatio... 1 Austria Phase 2 (2008-2012) 160218569 2000000 https://ec.europa.eu//clima/ets/napInstallatio... 2 Austria Phase 3 (2013-2020) 160295499 1893669 https://ec.europa.eu//clima/ets/napInstallatio... 3 Belgium Phase 1 (2005-2007) 178690906 7653297 https://ec.europa.eu//clima/ets/napInstallatio... 4 Belgium Phase 2 (2008-2012) 283317829 9153852 https://ec.europa.eu//clima/ets/napInstallatio...","title":"Retrieving Installation Table Links"},{"location":"02-installation-allocations/#retrieving-installation-allocation-data","text":"In this section we'll start by separating the root url and the parameters from each of the installation links #exports def get_url_root_and_params ( installations_link ): url_root = installations_link . split ( '?' )[ 0 ] parsed = urlparse . urlparse ( installations_link ) params = { k : v [ 0 ] for k , v in parse_qs ( parsed . query ) . items ()} return url_root , params installations_link = df_search . loc [ 0 , 'installations_link' ] url_root , params = get_url_root_and_params ( installations_link ) JSON ( params ) <IPython.core.display.JSON object> We also need to pass the page number to the parameters, to do this we need to first know how many pages there are #exports def get_num_pages ( root_url , params ): soup = bs ( requests . get ( root_url , params = params ) . text ) soup_pn = soup . find ( 'input' , attrs = { 'name' : 'resultList.lastPageNumber' }) if soup_pn is not None : num_pages = int ( soup_pn [ 'value' ]) else : num_pages = 1 return num_pages num_pages = get_num_pages ( root_url , params ) num_pages 11 We're now ready to iterate over multiple pages and combine the results for a single ETS phase in a single country #exports def extract_installation_allocations_df ( r ): soup = bs ( r . text ) table = soup . find ( 'table' , attrs = { 'id' : 'tblNapList' }) df_installation_allocations = ( pd . read_html ( str ( table )) [ 0 ] . drop ([ 0 , 1 ]) . reset_index ( drop = True ) . T . set_index ( 0 ) . T . drop ( columns = [ 'Options' ]) ) return df_installation_allocations def retry_request ( root_url , params , n_retries = 5 , ** kwargs ): for i in range ( n_retries ): try : r = requests . get ( root_url , params = params , ** kwargs ) return r except Exception as e : continue raise e def get_installation_allocations_df ( root_url , params , n_retries = 5 ): df_installation_allocations = pd . DataFrame () num_pages = get_num_pages ( root_url , params ) params [ 'nextList' ] = 'Next' for page_num in range ( num_pages ): params [ 'resultList.currentPageNumber' ] = page_num r = retry_request ( root_url , params , n_retries = n_retries ) df_installation_allocations_page = extract_installation_allocations_df ( r ) df_installation_allocations = df_installation_allocations . append ( df_installation_allocations_page ) df_installation_allocations = df_installation_allocations . reset_index ( drop = True ) return df_installation_allocations df_installation_allocations = get_installation_allocations_df ( root_url , params ) print ( f 'DataFrame shape: { df_installation_allocations . shape } ' ) df_installation_allocations . head ( 3 ) DataFrame shape: (201, 11) Installation ID Installation Name Address City Account Holder Name Account Status Permit ID Latest Update 2005 2006 2007 Status 0 1 Baumit Baustoffe Bad Ischl Bad Ischl Calmit GmbH open IKA119 2009-05-08 09:13:58 44894 44894 44894 Active 1 2 Breitenfelder Edelstahl Mitterdorf Mitterdorf Breitenfeld Edelstahl AG open IES069 2009-05-08 09:13:58 8492 8492 8492 Active 2 3 Ziegelwerk Danreiter Ried im Innkreis Ried Ziegelwerk Danreiter GmbH & Co KG open IZI155 2009-05-08 09:13:58 7397 7397 7397 Active The next step is to repeat this for all countries and ETS phases, then combine the resulting dataframes #exports def get_all_installation_allocations_df ( df_search ): col_renaming_map = { 'Installation ID' : 'installation_id' , 'Installation Name' : 'installation_name' , 'Address City' : 'installation_city' , 'Account Holder Name' : 'account_holder' , 'Account Status' : 'account_status' , 'Permit ID' : 'permit_id' , 'Status' : 'status' } df_installation_allocations = pd . DataFrame () # Retrieving raw data for country in track ( df_search [ 'country' ] . unique ()): df_installation_allocations_country = pd . DataFrame () country_installations_links = df_search . loc [ df_search [ 'country' ] == country , 'installations_link' ] for installations_link in track ( country_installations_links , label = country ): url_root , params = get_url_root_and_params ( installations_link ) df_installation_allocations_country_phase = get_installation_allocations_df ( root_url , params ) if df_installation_allocations_country . size > 0 : df_installation_allocations_country = pd . merge ( df_installation_allocations_country , df_installation_allocations_country_phase , how = 'outer' , on = list ( col_renaming_map . keys ())) else : df_installation_allocations_country = df_installation_allocations_country_phase df_installation_allocations_country [ 'country' ] = country df_installation_allocations = df_installation_allocations . append ( df_installation_allocations_country ) # Collating update datetimes update_cols = df_installation_allocations . columns [ df_installation_allocations . columns . str . contains ( 'Latest Update' )] df_installation_allocations [ 'latest_update' ] = df_installation_allocations [ update_cols ] . fillna ( '' ) . max ( axis = 1 ) df_installation_allocations = df_installation_allocations . drop ( columns = update_cols ) # Renaming columns df_installation_allocations = ( df_installation_allocations . reset_index ( drop = True ) . rename ( columns = col_renaming_map ) ) # Sorting column order non_year_cols = [ 'country' ] + list ( col_renaming_map . values ()) + [ 'latest_update' ] year_cols = sorted ( list ( set ( df_installation_allocations . columns ) - set ( non_year_cols ))) df_installation_allocations = df_installation_allocations [ non_year_cols + year_cols ] # Dropping header rows idxs_to_drop = df_installation_allocations [ 'permit_id' ] . str . contains ( '\\*' ) . replace ( False , np . nan ) . dropna () . index df_installation_allocations = df_installation_allocations . drop ( idxs_to_drop ) return df_installation_allocations df_installation_allocations = get_all_installation_allocations_df ( df_search ) df_installation_allocations . head () 100% 32/32 [24:25 < 01:34, 45.78s/it] Austria 100% 3/3 [00:16 < 00:05, 5.35s/it] Belgium 100% 3/3 [00:24 < 00:10, 7.95s/it] Bulgaria 100% 2/2 [00:07 < 00:03, 3.37s/it] Croatia 100% 1/1 [00:01 < 00:01, 1.26s/it] Cyprus 100% 2/2 [00:01 < 00:01, 0.64s/it] Czech Republic 100% 3/3 [00:33 < 00:12, 11.09s/it] Denmark 100% 3/3 [00:30 < 00:11, 10.07s/it] Estonia 100% 3/3 [00:04 < 00:01, 1.21s/it] Finland 100% 3/3 [01:04 < 00:19, 21.32s/it] France 100% 3/3 [02:50 < 01:18, 56.52s/it] Germany 100% 3/3 [06:22 < 03:02, 127.24s/it] Greece 100% 3/3 [00:09 < 00:03, 3.03s/it] Hungary 100% 3/3 [00:18 < 00:07, 5.88s/it] Iceland 100% 1/1 [00:01 < 00:01, 0.63s/it] Ireland 100% 3/3 [00:08 < 00:02, 2.58s/it] Italy 100% 3/3 [02:45 < 01:22, 54.94s/it] Latvia 100% 3/3 [00:06 < 00:02, 1.88s/it] Liechtenstein 100% 2/2 [00:01 < 00:00, 0.49s/it] Lithuania 100% 3/3 [00:07 < 00:03, 2.46s/it] Luxembourg 100% 3/3 [00:02 < 00:01, 0.54s/it] Malta 100% 1/1 [00:00 < 00:00, 0.47s/it] Netherlands 100% 3/3 [00:33 < 00:15, 10.85s/it] Northern Ireland 100% 3/3 [00:01 < 00:00, 0.46s/it] Norway 100% 2/2 [00:06 < 00:03, 3.00s/it] Poland 100% 3/3 [01:48 < 00:41, 36.07s/it] Portugal 100% 3/3 [00:16 < 00:05, 5.37s/it] Romania 100% 3/3 [00:16 < 00:05, 5.45s/it] Slovakia 100% 3/3 [00:11 < 00:04, 3.60s/it] Slovenia 100% 3/3 [00:05 < 00:01, 1.74s/it] Spain 100% 3/3 [02:11 < 00:48, 43.72s/it] Sweden 100% 3/3 [01:35 < 00:30, 31.67s/it] United Kingdom 100% 3/3 [01:34 < 00:31, 31.37s/it] country installation_id installation_name installation_city account_holder account_status permit_id status latest_update 2005 ... 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 0 Austria 1 Baumit Baustoffe Bad Ischl Bad Ischl Calmit GmbH open IKA119 Active 2013-12-19 15:47:52 44894 ... 43171 43171 42159 41426 40685 39937 39180 38416 37643 36866 1 Austria 2 Breitenfelder Edelstahl Mitterdorf Mitterdorf Breitenfeld Edelstahl AG open IES069 Active 2013-12-19 15:48:16 8492 ... 26429 26429 15118 14856 14590 14322 14051 13776 13499 13221 2 Austria 3 Ziegelwerk Danreiter Ried im Innkreis Ried Ziegelwerk Danreiter GmbH & Co KG open IZI155 Active 2013-12-19 15:48:12 7397 ... 5927 5927 3494 3434 3373 3311 3248 3185 3120 3056 3 Austria 4 Wienerberger Blindenmarkt Blindenmarkt Wienerberger \u00d6sterreich GmbH closed IZI146-1 Account Closed 2009-05-08 09:13:58 8335 ... nan nan nan nan nan nan nan nan nan nan 4 Austria 5 Isomax Dekorative Laminate Wiener Neudorf Wiener Neudorf FunderMax GmbH open ICH113 Active 2013-12-19 15:47:46 24003 ... 27343 27343 26223 25697 25167 24636 24102 23566 23026 22488 A quick check reveals some issues with the dataset, for example there are several permit ids that have duplicate entries permit_ids_with_duplicate_idxs = ( df_installation_allocations [ 'permit_id' ] . value_counts () > 1 ) . replace ( False , np . nan ) . dropna () . index df_dupe_permit_ids = df_installation_allocations [ df_installation_allocations [ 'permit_id' ] . isin ( permit_ids_with_duplicate_idxs )] . sort_values ( 'permit_id' ) print ( f \"There are { df_dupe_permit_ids [ 'permit_id' ] . unique () . size } permit ids which have duplicate entries due to inconsistent values (e.g. for `installation_id`) \\n \" ) df_dupe_permit_ids . head ( 6 ) There are 109 permit ids which have duplicate entries due to inconsistent values (e.g. for `installation_id`) country installation_id installation_name installation_city account_holder account_status permit_id status latest_update 2005 ... 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 4184 France 204319 ARKEMA FRANCE- Usine de Mont Orthez ARKEMA FRANCE open 5202690 Active 2014-01-27 16:54:18 nan ... nan nan 26157 25703 25242 24779 24308 23835 23355 22873 2937 France 102 ARKEMA FRANCE - Usine de Mont MONT ARKEMA FRANCE closed 5202690 Account Closed 2012-11-29 14:18:52 32802 ... 0 0 nan nan nan nan nan nan nan nan 4165 France 204063 SOCIETE FROMAGERE DE SAINTE CECILE SAINTE-CECILE SOCIETE FROMAGERE DE SAINTE CECILE open 5301510 Active 2014-01-27 16:57:52 nan ... nan nan 3864 3458 3063 2680 2309 1949 1602 1267 3913 France 1121 SOCIETE FROMAGERE DE SAINTE CECILE Sainte C\u00e9cile BOUVIER closed 5301510 Account Closed 2009-05-08 09:13:58 7790 ... nan nan nan nan nan nan nan nan nan nan 4162 France 204060 SOCIETE FROMAGERE DE DOMFRONT Domfront SOCIETE FROMAGERE DE DOMFRONT open 5302209 Active 2014-01-27 16:54:24 nan ... nan nan 5780 5173 4582 4009 3454 2916 2396 1895 2961 France 127 SOCIETE FROMAGERE DE DOMFRONT Domfront SCOTTO closed 5302209 Account Closed 2009-05-08 09:13:58 11326 ... nan nan nan nan nan nan nan nan nan nan Lastly we'll wrap these steps in a function that will retrieve/download our data to/from the specified directory #exports def get_installation_allocations_df ( data_dir = 'data' , redownload = False ): if not os . path . exists ( data_dir ): os . makedirs ( data_dir ) if redownload == True : df_search = get_installation_links_dataframe () df_installation_allocations = get_all_installation_allocations_df ( df_search ) df_installation_allocations . to_csv ( f ' { data_dir } /installation_allocations.csv' , index = False ) else : df_installation_allocations = pd . read_csv ( f ' { data_dir } /installation_allocations.csv' ) return df_installation_allocations redownload_installation_allocations = False df_installation_allocations = get_installation_allocations_df ( data_dir = '../data' , redownload = redownload_installation_allocations )","title":"Retrieving Installation Allocation Data"},{"location":"03-EUTL-database/","text":"EU Transaction Log Database \u00b6 import pandas as pd import numpy as np import xarray as xr import re import os import copy import requests from bs4 import BeautifulSoup as bs import matplotlib.pyplot as plt from ipypb import track import FEAutils as hlp from IPython.display import JSON Identifying Operator Holding Accounts \u00b6 We'll start by calling the first page of the unfiltered search #exports get_accounts_raw_search = lambda page_num = 0 : requests . get ( f 'https://ec.europa.eu/clima/ets/oha.do?form=oha&languageCode=en&accountHolder=&installationIdentifier=&installationName=&permitIdentifier=&mainActivityType=-1&searchType=oha&currentSortSettings=&backList=%3CBack&resultList.currentPageNumber= { page_num } ' ) r = get_accounts_raw_search () r <Response [200]> For this single page we'll create a function that converts the relevant table into a dataframe #exports def extract_search_df ( r ): soup = bs ( r . text ) results_table = soup . find ( 'table' , attrs = { 'id' : 'tblAccountSearchResult' }) df_search = ( pd . read_html ( str ( results_table )) [ 0 ] . iloc [ 1 :, : - 2 ] . reset_index ( drop = True ) . T . set_index ( 0 ) . T . reset_index ( drop = True ) . rename ( columns = { 'National Administrator' : 'country' , 'Account Type' : 'account_type' , 'Account Holder Name' : 'account_holder_name' , 'Installation/Aircraft ID' : 'installation_or_aircraft_id' , 'Installation Name/Aircraft Operator Code*' : 'operator_code' , 'Company Registration No' : 'company_registration_number' , 'Permit/Plan ID' : 'permit_or_plan_id' , 'Permit/Plan Date' : 'permit_or_plan_date' , 'Main Activity Type' : 'main_activity_type' , 'Latest Compliance Code' : 'latest_compliance_code' }) ) df_search [ 'account_id' ] = [ a [ 'href' ] . split ( 'accountID=' )[ - 1 ] . split ( '&' )[ 0 ] for a in results_table . findAll ( 'a' , text = re . compile ( 'Details - All Phases' ))] return df_search df_search = extract_search_df ( r ) df_search . head ( 3 ) country account_type account_holder_name installation_or_aircraft_id operator_code company_registration_number permit_or_plan_id permit_or_plan_date main_activity_type latest_compliance_code account_id 0 Austria Aircraft Operator Account Jetalliance Flugbetriebs GmbH 200103 27702 FN 203001g BMLFUW-UW.1.3.2/0354-V/4/2009 2010-01-01 Aircraft operator activities C 90574 1 Austria Aircraft Operator Account Glock GmbH 200108 194 FN64142b BMFLUW-UW.1.3.2/0084-V/4/2010 2010-01-01 Aircraft operator activities A 90727 2 Austria Aircraft Operator Account Glock Services GmbH 200109 36057 FN329154a UW.1.3.2/0085-V/4/2011 2010-01-01 Aircraft operator activities A 90728 We next need to identify how many pages we need to retrieve data from #exports def get_num_operating_accounts_pages (): r = get_accounts_raw_search () soup = bs ( r . text ) soup_pn = soup . find ( 'input' , attrs = { 'name' : 'resultList.lastPageNumber' }) num_pages = int ( soup_pn [ 'value' ]) return num_pages num_pages = get_num_operating_accounts_pages () num_pages 883 We're now ready to download and collate all of the separate pages #exports def get_full_search_df ( num_pages ): df_search = pd . DataFrame () for page_num in track ( range ( num_pages ), label = 'Accounts' ): r = get_accounts_raw_search ( page_num = page_num ) df_search_page = extract_search_df ( r ) df_search = df_search . append ( df_search_page ) df_search = ( df_search . reset_index ( drop = True ) . drop_duplicates () ) return df_search def get_search_df ( data_dir = 'data' , num_pages = None , redownload = False ): if num_pages is None : num_pages = get_num_operating_accounts_pages () if not os . path . exists ( data_dir ): os . makedirs ( data_dir ) if redownload == True : df_search = get_full_search_df ( num_pages ) df_search . to_csv ( f ' { data_dir } /account_search.csv' , index = False ) else : df_search = pd . read_csv ( f ' { data_dir } /account_search.csv' ) redownload_search_df = False df_search = get_search_df ( data_dir = '../data' , redownload = redownload_search_df ) df_search . shape 100% 880/883 [09:19 < 00:01, 0.63s/it] (17620, 11) We would intuitively expect to see only unique values in the permit_or_plan_id column but we can see for values such as EEW012 that this is not the case. In this instance it appears as though the account holder name has changed, meaning that the emissions data has been split despite it being owned by the same overarching company - these are problems we will have to address later. df_search . loc [ df_search [ 'permit_or_plan_id' ] == 'EEW012' ] country account_type account_holder_name installation_or_aircraft_id operator_code company_registration_number permit_or_plan_id permit_or_plan_date main_activity_type latest_compliance_code account_id 137 Austria Operator Holding Account VERBUND Thermal Power GmbH & Co KG 97 Verbund KW Voitsberg FN 220426 g EEW012 2004-01-02 Combustion installations with a rated thermal ... C 93794 271 Austria Operator Holding Account A-Tec Beteiligungs GmbH 233 DKW Voitsberg FN 294601 m EEW012 2005-06-09 Combustion installations with a rated thermal ... A 93944 We'll quickly inspect the number of each type of account df_search [ 'account_type' ] . value_counts () Operator Holding Account 16040 Aircraft Operator Account 1580 Name: account_type, dtype: int64 Retrieving Installation/Operator Data \u00b6 In this section we'll start looking at individual operator accounts and extracting some more detailed information #exports account_id_to_url = lambda account_id : f 'https://ec.europa.eu/clima/ets/ohaDetails.do?accountID= { account_id } &action=all' account_id = df_search . loc [ df_search [ 'account_type' ] == 'Aircraft Operator Account' , 'account_id' ] . iloc [ 0 ] account_url = account_id_to_url ( account_id ) account_url 'https://ec.europa.eu/clima/ets/ohaDetails.do?accountID=90574&action=all' We'll create a function that extracts the relevant tables as Beautiful Soup objects #exports def retry_request ( root_url , params = {}, n_retries = 10 , ** kwargs ): for i in range ( n_retries ): try : r = requests . get ( root_url , params = params , ** kwargs ) return r except Exception as e : err = e continue raise err def extract_key_table_soups ( account_url ): r = retry_request ( account_url ) soup = bs ( r . text ) operator_master_table = soup . find ( 'table' , attrs = { 'summary' : 'Master account details' }) operator_child_table , compliance_table = soup . findAll ( 'table' , attrs = { 'summary' : 'Child account details' }) return operator_master_table , operator_child_table , compliance_table operator_master_table , operator_child_table , compliance_table = extract_key_table_soups ( account_url ) len ( operator_master_table ), len ( operator_child_table ), len ( compliance_table ) (5, 5, 3) The first table we'll extract is the time-series compliance data def try_convert ( value , default , type_ ): try : return type_ ( value ) except : pass return default filter_for_year_indexes = lambda df : df . loc [ pd . Series ( df . index ) . apply ( try_convert , args = ( np . nan , float )) . dropna () . astype ( int ) . astype ( str ) . values ] def extract_compliance_df ( compliance_table ): df_compliance = ( pd . read_html ( str ( compliance_table ))[ 1 ] . iloc [ 1 :, : - 2 ] . reset_index ( drop = True ) . T . set_index ( 0 ) . T . reset_index ( drop = True ) . drop ( columns = [ 'EU ETS Phase' , 'Cumulative Surrendered Units**' , 'Cumulative Verified Emissions***' ]) . set_index ( 'Year' ) . pipe ( filter_for_year_indexes ) . rename ( columns = { 'Allowances in Allocation' : 'allocated_allowances' , 'Verified Emissions' : 'verified_emissions' , 'Units Surrendered' : 'units_surrendered' , 'Compliance Code' : 'compliance_code' }) ) return df_compliance df_compliance = extract_compliance_df ( compliance_table ) df_compliance . head () ('Unnamed: 0_level_0', 'Year') ('allocated_allowances', 'Unnamed: 1_level_1') ('verified_emissions', 'Unnamed: 2_level_1') ('units_surrendered', 'Unnamed: 3_level_1') ('compliance_code', 'Unnamed: 4_level_1') 2005 8492 12104 12104 A 2006 8492 12256 12256 A 2007 8492 14976 14976 A 2008 14063 17523 17523 A* 2009 17155 16815 16815 A We'll quickly visualise the cumulative excess emissions for the account we've just retrieved. When the cumulative emissions are below 0 we can imagine the account as being in credit, whereas when its above 0 we can view it as being in debt. df_plot = df_compliance [[ 'allocated_allowances' , 'verified_emissions' ]] . astype ( float ) . astype ( 'Int64' ) . dropna ( how = 'all' ) df_plot . index = df_plot . index . astype ( int ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ( df_plot [ 'verified_emissions' ] - df_plot [ 'allocated_allowances' ]) . cumsum () . plot ( ax = ax ) ax . plot ([ df_plot . index . min (), df_plot . index . max ()], [ 0 , 0 ], 'k--' ) ax . set_title ( f 'Account ID: { account_id } ' ) ax . set_ylabel ( 'Cumulative Excess Emissions' ) ax . set_xlabel ( '' ) ax . set_xlim ( df_plot . index . min (), df_plot . index . max ()) hlp . hide_spines ( ax ) We'll take a quick look at how well the tables can be parsed into dataframes using pd.read_html alone _ , df_master_general_info , _ , df_contact_info = pd . read_html ( str ( operator_master_table )) _ , df_child_general_info , df_address_info = pd . read_html ( str ( operator_child_table )) df_master_general_info 0 1 2 3 4 5 6 0 General Information General Information General Information General Information General Information General Information General Information 1 National Administrator Account Type Account Holder Name Installation ID Company Registration No Account Status nan 2 Austria 100-Holding Account Breitenfeld Edelstahl AG 2 FN 74471 t open nan We'll create some helper functions for extracting data from the different tables individually #exports extract_single_row_table_info = lambda df_info , num_excess_start_cols , last_end_col : df_info . iloc [ num_excess_start_cols :, : last_end_col ] . reset_index ( drop = True ) . T . set_index ( 0 )[ 1 ] . to_dict () extract_master_general_info = lambda df_master_general_info : extract_single_row_table_info ( df_master_general_info , 1 , 6 ) extract_child_general_info = lambda df_child_general_info : extract_single_row_table_info ( df_child_general_info , 1 , 10 ) extract_contact_info = lambda df_contact_info : extract_single_row_table_info ( df_contact_info , 1 , 11 ) extract_address_info = lambda df_address_info : extract_single_row_table_info ( df_address_info , 1 , 8 ) extract_aircraft_master_general_info ( df_master_general_info ) {'National Administrator': 'Austria', 'Account Type': '100-Holding Account', 'Account Holder Name': 'Breitenfeld Edelstahl AG', 'Installation ID': '2', 'Company Registration No': 'FN 74471 t', 'Account Status': 'open'} We'll now repeat this for all four of the table types and then combine them in a single dictionary #exports def clean_dict_2nd_level_nulls ( dict_ ): dict_ = { k1 : { k2 : ( v2 if v2 not in [ np . nan , 'nan' , '-' ] else None ) for k2 , v2 in v1 . items () } for k1 , v1 in dict_ . items () } return dict_ def extract_page_info ( account_id , master_general_info_func = extract_master_general_info , child_general_info_func = extract_child_general_info , ): # Retrieving table html account_url = account_id_to_url ( account_id ) operator_master_table , operator_child_table , compliance_table = extract_key_table_soups ( account_url ) # Extracting raw dataframes _ , df_master_general_info , _ , df_contact_info = pd . read_html ( str ( operator_master_table )) _ , df_child_general_info , df_address_info = pd . read_html ( str ( operator_child_table )) # Parsing to clean dictionaries master_general_info = master_general_info_func ( df_master_general_info ) child_general_info = child_general_info_func ( df_child_general_info ) contact_info = extract_contact_info ( df_contact_info ) address_info = extract_address_info ( df_address_info ) # Collating data page_info = { 'master_general_info' : master_general_info , 'child_general_info' : child_general_info , 'contact_info' : contact_info , 'address_info' : address_info } # Cleaning null values page_info = clean_dict_2nd_level_nulls ( page_info ) # Extracting time-series data df_ts = extract_compliance_df ( compliance_table ) return page_info , df_ts aircraft_page_info , df_ts = extract_page_info ( account_id ) JSON ( aircraft_page_info ) <IPython.core.display.JSON object> We can use the same function to extract the information from an installation page as well. N.b. this is only possible because the tables are of the same size and format, if this changes in future the aircraft and installation pages will need separate extraction functions account_id = df_search . loc [ df_search [ 'account_type' ] == 'Operator Holding Account' , 'account_id' ] . iloc [ 0 ] page_info , df_ts = extract_page_info ( account_id ) JSON ( installation_page_info ) <IPython.core.display.JSON object> We'll quickly write some helper functions for collating this data in separate owner and unit dictionaries #exports def collate_owner_info ( installation_page_info ): general_info = installation_page_info [ 'master_general_info' ] contact_info = installation_page_info [ 'contact_info' ] owner_info = copy . deepcopy ( general_info ) owner_info . update ( copy . deepcopy ( contact_info )) assert len ( general_info ) + len ( contact_info ) - len ( owner_info ) == 0 , 'There are duplicate entries in the dictionary keys' return owner_info def collate_unit_info ( installation_page_info ): general_info = installation_page_info [ 'child_general_info' ] address_info = installation_page_info [ 'address_info' ] installation_info = copy . deepcopy ( general_info ) installation_info . update ( copy . deepcopy ( address_info )) assert len ( general_info ) + len ( address_info ) - len ( installation_info ) == 0 , 'There are duplicate entries in the dictionary keys' return installation_info s_owner_info = pd . Series ( collate_owner_info ( page_info )) s_installation_info = pd . Series ( collate_unit_info ( page_info )) s_owner_info National Administrator Austria Account Type 100-Holding Account Account Holder Name Jetalliance Flugbetriebs GmbH Aircraft Operator ID 200103 Company Registration No FN 203001g Account Status closed Type Account holder Name Jetalliance Flugbetriebs GmbH Legal Entity Identifier None Main Address Line Flugplatz 1 Secondary Address Line None Postal Code 2542 City Kottingbrunn Country Austria Telephone 1 None Telephone 2 None E-Mail Address None dtype: object We'll create a wrapper for downloading and saving the installation data #exports def construct_ets_unit_dfs ( account_ids , owners_col_rename_map , units_col_rename_map , label = None ): df_owners = pd . DataFrame ( index = account_ids , columns = owners_col_rename_map . keys ()) df_units = pd . DataFrame ( index = account_ids , columns = units_col_rename_map . keys ()) ts_dfs = {} for account_id in track ( account_ids , label = label ): page_info , df_ts = extract_page_info ( account_id ) df_owners . loc [ account_id ] = pd . Series ( collate_owner_info ( page_info )) df_units . loc [ account_id ] = pd . Series ( collate_unit_info ( page_info )) ts_dfs [ account_id ] = df_ts df_owners = df_owners . rename ( columns = owners_col_rename_map ) df_units = df_units . rename ( columns = units_col_rename_map ) return df_owners , df_units , ts_dfs def constuct_da_ts_from_ts_dfs ( ts_dfs ): arr = np . stack ([ df . values for df in ts_dfs . values ()]) coords = { 'account_id' : list ( ts_dfs . keys ()), 'year' : list ( ts_dfs . values ())[ 0 ] . index . values , 'variable' : list ( ts_dfs . values ())[ 0 ] . columns . values } da_ts = xr . DataArray ( arr , coords = coords , dims = coords . keys ()) return da_ts def ts_dfs_to_separate_vars ( ts_dfs ): da_ts = constuct_da_ts_from_ts_dfs ( ts_dfs ) ts_var_dfs = {} for variable in da_ts [ 'variable' ] . values : ts_var_dfs [ variable ] = ( da_ts . sel ( variable = variable ) . to_dataframe ( name = variable ) [ variable ] . reset_index () . pivot ( 'account_id' , 'year' , variable ) ) return ts_var_dfs def construct_installation_dfs ( account_ids ): installation_owners_col_rename_map = { 'National Administrator' : 'national_administrator' , 'Account Type' : 'account_type' , 'Account Holder Name' : 'account_holder_name' , 'Installation ID' : 'installation_id' , 'Company Registration No' : 'company_registration_number' , 'Account Status' : 'account_status' , 'Type' : 'type' , 'Name' : 'name' , 'Legal Entity Identifier' : 'legal_entity_identifier' , 'Main Address Line' : 'first_address_line' , 'Secondary Address Line' : 'second_address_line' , 'Postal Code' : 'postcode' , 'City' : 'city' , 'Country' : 'country' , 'Telephone 1' : 'telephone_1' , 'Telephone 2' : 'telephone_2' , 'E-Mail Address' : 'email' } installations_col_rename_map = { 'Installation ID' : 'installation_id' , 'Installation Name' : 'installation_name' , 'Permit ID' : 'permit_id' , 'Permit Entry Date' : 'permit_entry_date' , 'Permit Expiry/Revocation Date' : 'permit_expiration_Date' , 'Name of Subsidiary undertaking' : 'subsidiary_undertaking_name' , 'Name of Parent undertaking' : 'parent_undertaking_name' , 'E-PRTR identification' : 'EPRTR_id' , 'First Year of Emissions' : 'initial_emissions_year' , 'Last Year of Emissions' : 'final_emissions_year' , 'Main Address Line' : 'first_address_line' , 'Secondary Address Line' : 'second_address_line' , 'Postal Code' : 'postcode' , 'City' : 'city' , 'Country' : 'country' , 'Latitude' : 'lat' , 'Longitude' : 'lon' , 'Main Activity' : 'main_activity' } df_owners , df_installations , ts_dfs = construct_ets_unit_dfs ( account_ids , installation_owners_col_rename_map , installations_col_rename_map , label = 'Installations' ) installation_dfs = ts_dfs_to_separate_vars ( ts_dfs ) installation_dfs . update ({ 'owners' : df_owners , 'installations' : df_installations }) return installation_dfs def get_installation_dfs ( df_search , data_dir = 'data/installations' , redownload = False ): df_search_installations = df_search . query ( \"account_type=='Operator Holding Account'\" ) account_ids = df_search_installations [ 'account_id' ] if not os . path . exists ( data_dir ): os . makedirs ( data_dir ) if redownload == True : installation_dfs = construct_installation_dfs ( account_ids ) for filename , df_installation in installation_dfs . items (): df_installation . to_csv ( f ' { data_dir } / { filename } .csv' ) else : installation_dfs = dict () filenames = [ f [: - 4 ] for f in os . listdir ( data_dir ) if '.csv' in f ] for filename in filenames : installation_dfs [ filename ] = pd . read_csv ( f ' { data_dir } / { filename } .csv' ) return installation_dfs redownload_installations = False installation_dfs = get_installation_dfs ( df_search , data_dir = '../data/installations' , redownload = redownload_installations ) installation_dfs [ 'owners' ] . shape 100% 16000/16040 [02:20:00 < 00:00, 0.52s/it] (16040, 17) We can do the same for the aircraft data as well #exports def construct_aircraft_dfs ( account_ids ): aircraft_owners_col_rename_map = { 'National Administrator' : 'national_administrator' , 'Account Type' : 'account_type' , 'Account Holder Name' : 'account_holder_name' , 'Aircraft Operator ID' : 'aircraft_operator_id' , 'Company Registration No' : 'company_registration_number' , 'Account Status' : 'account_status' , 'Type' : 'type' , 'Name' : 'name' , 'Legal Entity Identifier' : 'legal_entity_identifier' , 'Main Address Line' : 'first_address_line' , 'Secondary Address Line' : 'second_address_line' , 'Postal Code' : 'postcode' , 'City' : 'city' , 'Country' : 'country' , 'Telephone 1' : 'telephone_1' , 'Telephone 2' : 'telephone_2' , 'E-Mail Address' : 'email' } aircraft_col_rename_map = { 'Aircraft Operator ID' : 'aircraft_operator_id' , 'Unique Code under Commission Regulation (EC) No 748/2009' : '' , 'Monitoring Plan ID' : 'monitoring_plan_id' , 'Monitoring plan \u2014 first year of applicability' : 'monitoring_plan_start_date' , 'Monitoring plan \u2014 year of expiry' : 'monitoring_plan_expiration_Date' , 'Name of Subsidiary undertaking' : 'subsidiary_undertaking_name' , 'Name of Parent undertaking' : 'parent_undertaking_name' , 'E-PRTR identification' : 'EPRTR_id' , 'Call Sign (ICAO designator)' : 'call_sign' , 'First Year of Emissions' : 'initial_emissions_year' , 'Main Address Line' : 'first_address_line' , 'Secondary Address Line' : 'second_address_line' , 'Postal Code' : 'postcode' , 'City' : 'city' , 'Country' : 'country' , 'Latitude' : 'lat' , 'Longitude' : 'lon' , 'Main Activity' : 'main_activity' } df_owners , df_aircraft , ts_dfs = construct_ets_unit_dfs ( account_ids , aircraft_owners_col_rename_map , aircraft_col_rename_map , label = 'Aircraft' ) aircraft_dfs = ts_dfs_to_separate_vars ( ts_dfs ) aircraft_dfs . update ({ 'owners' : df_owners , 'aircraft' : df_aircraft }) return aircraft_dfs def get_aircraft_dfs ( df_search , data_dir = 'data/aircraft' , redownload = False ): df_search_aircraft = df_search . query ( \"account_type=='Aircraft Operator Account'\" ) account_ids = df_search_aircraft [ 'account_id' ] if not os . path . exists ( data_dir ): os . makedirs ( data_dir ) redownload_aircraft = True if redownload_aircraft == True : aircraft_dfs = construct_aircraft_dfs ( account_ids ) for filename , df_aircraft in aircraft_dfs . items (): df_aircraft . to_csv ( f ' { data_dir } / { filename } .csv' ) else : aircraft_dfs = dict () filenames = [ f [: - 4 ] for f in os . listdir ( data_dir ) if '.csv' in f ] for filename in filenames : aircraft_dfs [ filename ] = pd . read_csv ( f ' { data_dir } / { filename } .csv' ) return aircraft_dfs redownload_aircraft = False aircraft_dfs = get_aircraft_dfs ( df_search , data_dir = '../data/aircraft' , redownload = redownload_aircraft ) aircraft_dfs [ 'owners' ] . shape 100% 1575/1580 [14:53 < 00:01, 0.57s/it] (1580, 17) Putting It All Together \u00b6 Finally we'll finish by creating a wrapper that downloads all of the data to the specified directory #exports def redownload_all_data ( data_dir = 'data' ): all_dfs = dict () all_dfs [ 'account_search' ] = get_search_df ( data_dir = data_dir , redownload = True ) installation_dfs = get_installation_dfs ( all_dfs [ 'account_search' ], data_dir = f ' { data_dir } /installations' , redownload = True ) aircraft_dfs = get_aircraft_dfs ( all_dfs [ 'account_search' ], data_dir = f ' { data_dir } /aircraft' , redownload = True ) all_dfs . update ( installation_dfs ) all_dfs . update ( aircraft_dfs ) return all_dfs redownload_everything = False if redownload_everything == True : all_dfs = redownload_all_data ( data_dir = '../data' )","title":"Accounts"},{"location":"03-EUTL-database/#eu-transaction-log-database","text":"import pandas as pd import numpy as np import xarray as xr import re import os import copy import requests from bs4 import BeautifulSoup as bs import matplotlib.pyplot as plt from ipypb import track import FEAutils as hlp from IPython.display import JSON","title":"EU Transaction Log Database"},{"location":"03-EUTL-database/#identifying-operator-holding-accounts","text":"We'll start by calling the first page of the unfiltered search #exports get_accounts_raw_search = lambda page_num = 0 : requests . get ( f 'https://ec.europa.eu/clima/ets/oha.do?form=oha&languageCode=en&accountHolder=&installationIdentifier=&installationName=&permitIdentifier=&mainActivityType=-1&searchType=oha&currentSortSettings=&backList=%3CBack&resultList.currentPageNumber= { page_num } ' ) r = get_accounts_raw_search () r <Response [200]> For this single page we'll create a function that converts the relevant table into a dataframe #exports def extract_search_df ( r ): soup = bs ( r . text ) results_table = soup . find ( 'table' , attrs = { 'id' : 'tblAccountSearchResult' }) df_search = ( pd . read_html ( str ( results_table )) [ 0 ] . iloc [ 1 :, : - 2 ] . reset_index ( drop = True ) . T . set_index ( 0 ) . T . reset_index ( drop = True ) . rename ( columns = { 'National Administrator' : 'country' , 'Account Type' : 'account_type' , 'Account Holder Name' : 'account_holder_name' , 'Installation/Aircraft ID' : 'installation_or_aircraft_id' , 'Installation Name/Aircraft Operator Code*' : 'operator_code' , 'Company Registration No' : 'company_registration_number' , 'Permit/Plan ID' : 'permit_or_plan_id' , 'Permit/Plan Date' : 'permit_or_plan_date' , 'Main Activity Type' : 'main_activity_type' , 'Latest Compliance Code' : 'latest_compliance_code' }) ) df_search [ 'account_id' ] = [ a [ 'href' ] . split ( 'accountID=' )[ - 1 ] . split ( '&' )[ 0 ] for a in results_table . findAll ( 'a' , text = re . compile ( 'Details - All Phases' ))] return df_search df_search = extract_search_df ( r ) df_search . head ( 3 ) country account_type account_holder_name installation_or_aircraft_id operator_code company_registration_number permit_or_plan_id permit_or_plan_date main_activity_type latest_compliance_code account_id 0 Austria Aircraft Operator Account Jetalliance Flugbetriebs GmbH 200103 27702 FN 203001g BMLFUW-UW.1.3.2/0354-V/4/2009 2010-01-01 Aircraft operator activities C 90574 1 Austria Aircraft Operator Account Glock GmbH 200108 194 FN64142b BMFLUW-UW.1.3.2/0084-V/4/2010 2010-01-01 Aircraft operator activities A 90727 2 Austria Aircraft Operator Account Glock Services GmbH 200109 36057 FN329154a UW.1.3.2/0085-V/4/2011 2010-01-01 Aircraft operator activities A 90728 We next need to identify how many pages we need to retrieve data from #exports def get_num_operating_accounts_pages (): r = get_accounts_raw_search () soup = bs ( r . text ) soup_pn = soup . find ( 'input' , attrs = { 'name' : 'resultList.lastPageNumber' }) num_pages = int ( soup_pn [ 'value' ]) return num_pages num_pages = get_num_operating_accounts_pages () num_pages 883 We're now ready to download and collate all of the separate pages #exports def get_full_search_df ( num_pages ): df_search = pd . DataFrame () for page_num in track ( range ( num_pages ), label = 'Accounts' ): r = get_accounts_raw_search ( page_num = page_num ) df_search_page = extract_search_df ( r ) df_search = df_search . append ( df_search_page ) df_search = ( df_search . reset_index ( drop = True ) . drop_duplicates () ) return df_search def get_search_df ( data_dir = 'data' , num_pages = None , redownload = False ): if num_pages is None : num_pages = get_num_operating_accounts_pages () if not os . path . exists ( data_dir ): os . makedirs ( data_dir ) if redownload == True : df_search = get_full_search_df ( num_pages ) df_search . to_csv ( f ' { data_dir } /account_search.csv' , index = False ) else : df_search = pd . read_csv ( f ' { data_dir } /account_search.csv' ) redownload_search_df = False df_search = get_search_df ( data_dir = '../data' , redownload = redownload_search_df ) df_search . shape 100% 880/883 [09:19 < 00:01, 0.63s/it] (17620, 11) We would intuitively expect to see only unique values in the permit_or_plan_id column but we can see for values such as EEW012 that this is not the case. In this instance it appears as though the account holder name has changed, meaning that the emissions data has been split despite it being owned by the same overarching company - these are problems we will have to address later. df_search . loc [ df_search [ 'permit_or_plan_id' ] == 'EEW012' ] country account_type account_holder_name installation_or_aircraft_id operator_code company_registration_number permit_or_plan_id permit_or_plan_date main_activity_type latest_compliance_code account_id 137 Austria Operator Holding Account VERBUND Thermal Power GmbH & Co KG 97 Verbund KW Voitsberg FN 220426 g EEW012 2004-01-02 Combustion installations with a rated thermal ... C 93794 271 Austria Operator Holding Account A-Tec Beteiligungs GmbH 233 DKW Voitsberg FN 294601 m EEW012 2005-06-09 Combustion installations with a rated thermal ... A 93944 We'll quickly inspect the number of each type of account df_search [ 'account_type' ] . value_counts () Operator Holding Account 16040 Aircraft Operator Account 1580 Name: account_type, dtype: int64","title":"Identifying Operator Holding Accounts"},{"location":"03-EUTL-database/#retrieving-installationoperator-data","text":"In this section we'll start looking at individual operator accounts and extracting some more detailed information #exports account_id_to_url = lambda account_id : f 'https://ec.europa.eu/clima/ets/ohaDetails.do?accountID= { account_id } &action=all' account_id = df_search . loc [ df_search [ 'account_type' ] == 'Aircraft Operator Account' , 'account_id' ] . iloc [ 0 ] account_url = account_id_to_url ( account_id ) account_url 'https://ec.europa.eu/clima/ets/ohaDetails.do?accountID=90574&action=all' We'll create a function that extracts the relevant tables as Beautiful Soup objects #exports def retry_request ( root_url , params = {}, n_retries = 10 , ** kwargs ): for i in range ( n_retries ): try : r = requests . get ( root_url , params = params , ** kwargs ) return r except Exception as e : err = e continue raise err def extract_key_table_soups ( account_url ): r = retry_request ( account_url ) soup = bs ( r . text ) operator_master_table = soup . find ( 'table' , attrs = { 'summary' : 'Master account details' }) operator_child_table , compliance_table = soup . findAll ( 'table' , attrs = { 'summary' : 'Child account details' }) return operator_master_table , operator_child_table , compliance_table operator_master_table , operator_child_table , compliance_table = extract_key_table_soups ( account_url ) len ( operator_master_table ), len ( operator_child_table ), len ( compliance_table ) (5, 5, 3) The first table we'll extract is the time-series compliance data def try_convert ( value , default , type_ ): try : return type_ ( value ) except : pass return default filter_for_year_indexes = lambda df : df . loc [ pd . Series ( df . index ) . apply ( try_convert , args = ( np . nan , float )) . dropna () . astype ( int ) . astype ( str ) . values ] def extract_compliance_df ( compliance_table ): df_compliance = ( pd . read_html ( str ( compliance_table ))[ 1 ] . iloc [ 1 :, : - 2 ] . reset_index ( drop = True ) . T . set_index ( 0 ) . T . reset_index ( drop = True ) . drop ( columns = [ 'EU ETS Phase' , 'Cumulative Surrendered Units**' , 'Cumulative Verified Emissions***' ]) . set_index ( 'Year' ) . pipe ( filter_for_year_indexes ) . rename ( columns = { 'Allowances in Allocation' : 'allocated_allowances' , 'Verified Emissions' : 'verified_emissions' , 'Units Surrendered' : 'units_surrendered' , 'Compliance Code' : 'compliance_code' }) ) return df_compliance df_compliance = extract_compliance_df ( compliance_table ) df_compliance . head () ('Unnamed: 0_level_0', 'Year') ('allocated_allowances', 'Unnamed: 1_level_1') ('verified_emissions', 'Unnamed: 2_level_1') ('units_surrendered', 'Unnamed: 3_level_1') ('compliance_code', 'Unnamed: 4_level_1') 2005 8492 12104 12104 A 2006 8492 12256 12256 A 2007 8492 14976 14976 A 2008 14063 17523 17523 A* 2009 17155 16815 16815 A We'll quickly visualise the cumulative excess emissions for the account we've just retrieved. When the cumulative emissions are below 0 we can imagine the account as being in credit, whereas when its above 0 we can view it as being in debt. df_plot = df_compliance [[ 'allocated_allowances' , 'verified_emissions' ]] . astype ( float ) . astype ( 'Int64' ) . dropna ( how = 'all' ) df_plot . index = df_plot . index . astype ( int ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ( df_plot [ 'verified_emissions' ] - df_plot [ 'allocated_allowances' ]) . cumsum () . plot ( ax = ax ) ax . plot ([ df_plot . index . min (), df_plot . index . max ()], [ 0 , 0 ], 'k--' ) ax . set_title ( f 'Account ID: { account_id } ' ) ax . set_ylabel ( 'Cumulative Excess Emissions' ) ax . set_xlabel ( '' ) ax . set_xlim ( df_plot . index . min (), df_plot . index . max ()) hlp . hide_spines ( ax ) We'll take a quick look at how well the tables can be parsed into dataframes using pd.read_html alone _ , df_master_general_info , _ , df_contact_info = pd . read_html ( str ( operator_master_table )) _ , df_child_general_info , df_address_info = pd . read_html ( str ( operator_child_table )) df_master_general_info 0 1 2 3 4 5 6 0 General Information General Information General Information General Information General Information General Information General Information 1 National Administrator Account Type Account Holder Name Installation ID Company Registration No Account Status nan 2 Austria 100-Holding Account Breitenfeld Edelstahl AG 2 FN 74471 t open nan We'll create some helper functions for extracting data from the different tables individually #exports extract_single_row_table_info = lambda df_info , num_excess_start_cols , last_end_col : df_info . iloc [ num_excess_start_cols :, : last_end_col ] . reset_index ( drop = True ) . T . set_index ( 0 )[ 1 ] . to_dict () extract_master_general_info = lambda df_master_general_info : extract_single_row_table_info ( df_master_general_info , 1 , 6 ) extract_child_general_info = lambda df_child_general_info : extract_single_row_table_info ( df_child_general_info , 1 , 10 ) extract_contact_info = lambda df_contact_info : extract_single_row_table_info ( df_contact_info , 1 , 11 ) extract_address_info = lambda df_address_info : extract_single_row_table_info ( df_address_info , 1 , 8 ) extract_aircraft_master_general_info ( df_master_general_info ) {'National Administrator': 'Austria', 'Account Type': '100-Holding Account', 'Account Holder Name': 'Breitenfeld Edelstahl AG', 'Installation ID': '2', 'Company Registration No': 'FN 74471 t', 'Account Status': 'open'} We'll now repeat this for all four of the table types and then combine them in a single dictionary #exports def clean_dict_2nd_level_nulls ( dict_ ): dict_ = { k1 : { k2 : ( v2 if v2 not in [ np . nan , 'nan' , '-' ] else None ) for k2 , v2 in v1 . items () } for k1 , v1 in dict_ . items () } return dict_ def extract_page_info ( account_id , master_general_info_func = extract_master_general_info , child_general_info_func = extract_child_general_info , ): # Retrieving table html account_url = account_id_to_url ( account_id ) operator_master_table , operator_child_table , compliance_table = extract_key_table_soups ( account_url ) # Extracting raw dataframes _ , df_master_general_info , _ , df_contact_info = pd . read_html ( str ( operator_master_table )) _ , df_child_general_info , df_address_info = pd . read_html ( str ( operator_child_table )) # Parsing to clean dictionaries master_general_info = master_general_info_func ( df_master_general_info ) child_general_info = child_general_info_func ( df_child_general_info ) contact_info = extract_contact_info ( df_contact_info ) address_info = extract_address_info ( df_address_info ) # Collating data page_info = { 'master_general_info' : master_general_info , 'child_general_info' : child_general_info , 'contact_info' : contact_info , 'address_info' : address_info } # Cleaning null values page_info = clean_dict_2nd_level_nulls ( page_info ) # Extracting time-series data df_ts = extract_compliance_df ( compliance_table ) return page_info , df_ts aircraft_page_info , df_ts = extract_page_info ( account_id ) JSON ( aircraft_page_info ) <IPython.core.display.JSON object> We can use the same function to extract the information from an installation page as well. N.b. this is only possible because the tables are of the same size and format, if this changes in future the aircraft and installation pages will need separate extraction functions account_id = df_search . loc [ df_search [ 'account_type' ] == 'Operator Holding Account' , 'account_id' ] . iloc [ 0 ] page_info , df_ts = extract_page_info ( account_id ) JSON ( installation_page_info ) <IPython.core.display.JSON object> We'll quickly write some helper functions for collating this data in separate owner and unit dictionaries #exports def collate_owner_info ( installation_page_info ): general_info = installation_page_info [ 'master_general_info' ] contact_info = installation_page_info [ 'contact_info' ] owner_info = copy . deepcopy ( general_info ) owner_info . update ( copy . deepcopy ( contact_info )) assert len ( general_info ) + len ( contact_info ) - len ( owner_info ) == 0 , 'There are duplicate entries in the dictionary keys' return owner_info def collate_unit_info ( installation_page_info ): general_info = installation_page_info [ 'child_general_info' ] address_info = installation_page_info [ 'address_info' ] installation_info = copy . deepcopy ( general_info ) installation_info . update ( copy . deepcopy ( address_info )) assert len ( general_info ) + len ( address_info ) - len ( installation_info ) == 0 , 'There are duplicate entries in the dictionary keys' return installation_info s_owner_info = pd . Series ( collate_owner_info ( page_info )) s_installation_info = pd . Series ( collate_unit_info ( page_info )) s_owner_info National Administrator Austria Account Type 100-Holding Account Account Holder Name Jetalliance Flugbetriebs GmbH Aircraft Operator ID 200103 Company Registration No FN 203001g Account Status closed Type Account holder Name Jetalliance Flugbetriebs GmbH Legal Entity Identifier None Main Address Line Flugplatz 1 Secondary Address Line None Postal Code 2542 City Kottingbrunn Country Austria Telephone 1 None Telephone 2 None E-Mail Address None dtype: object We'll create a wrapper for downloading and saving the installation data #exports def construct_ets_unit_dfs ( account_ids , owners_col_rename_map , units_col_rename_map , label = None ): df_owners = pd . DataFrame ( index = account_ids , columns = owners_col_rename_map . keys ()) df_units = pd . DataFrame ( index = account_ids , columns = units_col_rename_map . keys ()) ts_dfs = {} for account_id in track ( account_ids , label = label ): page_info , df_ts = extract_page_info ( account_id ) df_owners . loc [ account_id ] = pd . Series ( collate_owner_info ( page_info )) df_units . loc [ account_id ] = pd . Series ( collate_unit_info ( page_info )) ts_dfs [ account_id ] = df_ts df_owners = df_owners . rename ( columns = owners_col_rename_map ) df_units = df_units . rename ( columns = units_col_rename_map ) return df_owners , df_units , ts_dfs def constuct_da_ts_from_ts_dfs ( ts_dfs ): arr = np . stack ([ df . values for df in ts_dfs . values ()]) coords = { 'account_id' : list ( ts_dfs . keys ()), 'year' : list ( ts_dfs . values ())[ 0 ] . index . values , 'variable' : list ( ts_dfs . values ())[ 0 ] . columns . values } da_ts = xr . DataArray ( arr , coords = coords , dims = coords . keys ()) return da_ts def ts_dfs_to_separate_vars ( ts_dfs ): da_ts = constuct_da_ts_from_ts_dfs ( ts_dfs ) ts_var_dfs = {} for variable in da_ts [ 'variable' ] . values : ts_var_dfs [ variable ] = ( da_ts . sel ( variable = variable ) . to_dataframe ( name = variable ) [ variable ] . reset_index () . pivot ( 'account_id' , 'year' , variable ) ) return ts_var_dfs def construct_installation_dfs ( account_ids ): installation_owners_col_rename_map = { 'National Administrator' : 'national_administrator' , 'Account Type' : 'account_type' , 'Account Holder Name' : 'account_holder_name' , 'Installation ID' : 'installation_id' , 'Company Registration No' : 'company_registration_number' , 'Account Status' : 'account_status' , 'Type' : 'type' , 'Name' : 'name' , 'Legal Entity Identifier' : 'legal_entity_identifier' , 'Main Address Line' : 'first_address_line' , 'Secondary Address Line' : 'second_address_line' , 'Postal Code' : 'postcode' , 'City' : 'city' , 'Country' : 'country' , 'Telephone 1' : 'telephone_1' , 'Telephone 2' : 'telephone_2' , 'E-Mail Address' : 'email' } installations_col_rename_map = { 'Installation ID' : 'installation_id' , 'Installation Name' : 'installation_name' , 'Permit ID' : 'permit_id' , 'Permit Entry Date' : 'permit_entry_date' , 'Permit Expiry/Revocation Date' : 'permit_expiration_Date' , 'Name of Subsidiary undertaking' : 'subsidiary_undertaking_name' , 'Name of Parent undertaking' : 'parent_undertaking_name' , 'E-PRTR identification' : 'EPRTR_id' , 'First Year of Emissions' : 'initial_emissions_year' , 'Last Year of Emissions' : 'final_emissions_year' , 'Main Address Line' : 'first_address_line' , 'Secondary Address Line' : 'second_address_line' , 'Postal Code' : 'postcode' , 'City' : 'city' , 'Country' : 'country' , 'Latitude' : 'lat' , 'Longitude' : 'lon' , 'Main Activity' : 'main_activity' } df_owners , df_installations , ts_dfs = construct_ets_unit_dfs ( account_ids , installation_owners_col_rename_map , installations_col_rename_map , label = 'Installations' ) installation_dfs = ts_dfs_to_separate_vars ( ts_dfs ) installation_dfs . update ({ 'owners' : df_owners , 'installations' : df_installations }) return installation_dfs def get_installation_dfs ( df_search , data_dir = 'data/installations' , redownload = False ): df_search_installations = df_search . query ( \"account_type=='Operator Holding Account'\" ) account_ids = df_search_installations [ 'account_id' ] if not os . path . exists ( data_dir ): os . makedirs ( data_dir ) if redownload == True : installation_dfs = construct_installation_dfs ( account_ids ) for filename , df_installation in installation_dfs . items (): df_installation . to_csv ( f ' { data_dir } / { filename } .csv' ) else : installation_dfs = dict () filenames = [ f [: - 4 ] for f in os . listdir ( data_dir ) if '.csv' in f ] for filename in filenames : installation_dfs [ filename ] = pd . read_csv ( f ' { data_dir } / { filename } .csv' ) return installation_dfs redownload_installations = False installation_dfs = get_installation_dfs ( df_search , data_dir = '../data/installations' , redownload = redownload_installations ) installation_dfs [ 'owners' ] . shape 100% 16000/16040 [02:20:00 < 00:00, 0.52s/it] (16040, 17) We can do the same for the aircraft data as well #exports def construct_aircraft_dfs ( account_ids ): aircraft_owners_col_rename_map = { 'National Administrator' : 'national_administrator' , 'Account Type' : 'account_type' , 'Account Holder Name' : 'account_holder_name' , 'Aircraft Operator ID' : 'aircraft_operator_id' , 'Company Registration No' : 'company_registration_number' , 'Account Status' : 'account_status' , 'Type' : 'type' , 'Name' : 'name' , 'Legal Entity Identifier' : 'legal_entity_identifier' , 'Main Address Line' : 'first_address_line' , 'Secondary Address Line' : 'second_address_line' , 'Postal Code' : 'postcode' , 'City' : 'city' , 'Country' : 'country' , 'Telephone 1' : 'telephone_1' , 'Telephone 2' : 'telephone_2' , 'E-Mail Address' : 'email' } aircraft_col_rename_map = { 'Aircraft Operator ID' : 'aircraft_operator_id' , 'Unique Code under Commission Regulation (EC) No 748/2009' : '' , 'Monitoring Plan ID' : 'monitoring_plan_id' , 'Monitoring plan \u2014 first year of applicability' : 'monitoring_plan_start_date' , 'Monitoring plan \u2014 year of expiry' : 'monitoring_plan_expiration_Date' , 'Name of Subsidiary undertaking' : 'subsidiary_undertaking_name' , 'Name of Parent undertaking' : 'parent_undertaking_name' , 'E-PRTR identification' : 'EPRTR_id' , 'Call Sign (ICAO designator)' : 'call_sign' , 'First Year of Emissions' : 'initial_emissions_year' , 'Main Address Line' : 'first_address_line' , 'Secondary Address Line' : 'second_address_line' , 'Postal Code' : 'postcode' , 'City' : 'city' , 'Country' : 'country' , 'Latitude' : 'lat' , 'Longitude' : 'lon' , 'Main Activity' : 'main_activity' } df_owners , df_aircraft , ts_dfs = construct_ets_unit_dfs ( account_ids , aircraft_owners_col_rename_map , aircraft_col_rename_map , label = 'Aircraft' ) aircraft_dfs = ts_dfs_to_separate_vars ( ts_dfs ) aircraft_dfs . update ({ 'owners' : df_owners , 'aircraft' : df_aircraft }) return aircraft_dfs def get_aircraft_dfs ( df_search , data_dir = 'data/aircraft' , redownload = False ): df_search_aircraft = df_search . query ( \"account_type=='Aircraft Operator Account'\" ) account_ids = df_search_aircraft [ 'account_id' ] if not os . path . exists ( data_dir ): os . makedirs ( data_dir ) redownload_aircraft = True if redownload_aircraft == True : aircraft_dfs = construct_aircraft_dfs ( account_ids ) for filename , df_aircraft in aircraft_dfs . items (): df_aircraft . to_csv ( f ' { data_dir } / { filename } .csv' ) else : aircraft_dfs = dict () filenames = [ f [: - 4 ] for f in os . listdir ( data_dir ) if '.csv' in f ] for filename in filenames : aircraft_dfs [ filename ] = pd . read_csv ( f ' { data_dir } / { filename } .csv' ) return aircraft_dfs redownload_aircraft = False aircraft_dfs = get_aircraft_dfs ( df_search , data_dir = '../data/aircraft' , redownload = redownload_aircraft ) aircraft_dfs [ 'owners' ] . shape 100% 1575/1580 [14:53 < 00:01, 0.57s/it] (1580, 17)","title":"Retrieving Installation/Operator Data"},{"location":"03-EUTL-database/#putting-it-all-together","text":"Finally we'll finish by creating a wrapper that downloads all of the data to the specified directory #exports def redownload_all_data ( data_dir = 'data' ): all_dfs = dict () all_dfs [ 'account_search' ] = get_search_df ( data_dir = data_dir , redownload = True ) installation_dfs = get_installation_dfs ( all_dfs [ 'account_search' ], data_dir = f ' { data_dir } /installations' , redownload = True ) aircraft_dfs = get_aircraft_dfs ( all_dfs [ 'account_search' ], data_dir = f ' { data_dir } /aircraft' , redownload = True ) all_dfs . update ( installation_dfs ) all_dfs . update ( aircraft_dfs ) return all_dfs redownload_everything = False if redownload_everything == True : all_dfs = redownload_all_data ( data_dir = '../data' )","title":"Putting It All Together"}]}